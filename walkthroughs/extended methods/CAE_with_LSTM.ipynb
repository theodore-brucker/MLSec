{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder with CNN and LSTM layers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access the environment variables and read in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_directory = os.getenv('SEQUENCE_DIRECTORY')\n",
    "sequence_dataset = os.getenv('SEQUENCE_DATASET')\n",
    "\n",
    "dataset_path = os.path.join(sequence_directory, sequence_dataset)\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_protocol(df, column_name='protocol'):\n",
    "    \"\"\"One-hot encode the protocol column of a pandas DataFrame.\"\"\"\n",
    "    one_hot = pd.get_dummies(df[column_name], prefix=column_name, dtype=float)\n",
    "    return pd.concat([df.drop(column_name, axis=1), one_hot], axis=1)\n",
    "\n",
    "def hash_ip(ip_address):\n",
    "    \"\"\"Hash an IP address into a fixed-size integer.\"\"\"\n",
    "    # Use SHA-256 hashing\n",
    "    hashed_ip = hashlib.sha256(ip_address.encode('utf-8')).hexdigest()\n",
    "    # Convert the hash to an integer\n",
    "    numeric_ip = int(hashed_ip, 16) % (10 ** 10)  # Use modulo to keep integer size manageable\n",
    "    return numeric_ip\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "df.sort_values(by='timing', inplace=True)\n",
    "\n",
    "df['src_ip'] = df['src_ip'].astype(str) \n",
    "df['dst_ip'] = df['dst_ip'].astype(str)\n",
    "df['src_ip'] = df['src_ip'].apply(hash_ip)\n",
    "df['dst_ip'] = df['dst_ip'].apply(hash_ip)\n",
    "\n",
    "# One-hot encode the protocol column\n",
    "df = one_hot_encode_protocol(df)\n",
    "\n",
    "print(df.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences from the DataFrame\n",
    "def create_sequences(df, sequence_length=32):\n",
    "    \"\"\"\n",
    "    Organize the DataFrame by time and create sequences of packets with the same label.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, the preprocessed DataFrame.\n",
    "    - sequence_length: int, the length of each sequence.\n",
    "\n",
    "    Returns:\n",
    "    - sequences: List of np.arrays, the packet sequences.\n",
    "    - sequence_labels: List of int, the labels for each sequence.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    sequence_labels = []\n",
    "    \n",
    "    # Step 2 & 3: Filter by Label and Create Sequences\n",
    "    for label in df['label'].unique():\n",
    "        label_df = df[df['label'] == label]\n",
    "        \n",
    "        # Split the dataframe into chunks of size `sequence_length`\n",
    "        num_sequences = len(label_df) // sequence_length\n",
    "        for i in range(num_sequences):\n",
    "            sequence = label_df.iloc[i*sequence_length : (i+1)*sequence_length]\n",
    "            sequences.append(sequence.drop(['label'], axis=1).to_numpy())\n",
    "            sequence_labels.append(label)\n",
    "    \n",
    "    return sequences, sequence_labels\n",
    "\n",
    "# Apply the function to create sequences\n",
    "sequences, sequence_labels = create_sequences(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_sequences(sequences):\n",
    "    \"\"\"\n",
    "    Scale the sequences to have a Gaussian distribution with a mean of 0 and a std of 1.\n",
    "\n",
    "    Parameters:\n",
    "    - sequences: List of np.arrays, the packet sequences.\n",
    "\n",
    "    Returns:\n",
    "    - scaled_sequences: List of np.arrays, the scaled packet sequences.\n",
    "    \"\"\"\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Reshape the sequences for scaling: Convert list of 2D arrays into a 2D array\n",
    "    sequences_shape = sequences[0].shape # Shape of a single sequence (16, num_features)\n",
    "    sequences_concatenated = np.concatenate(sequences, axis=0)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    scaled_data = scaler.fit_transform(sequences_concatenated)\n",
    "    \n",
    "    # Reshape back into the original list of 2D arrays\n",
    "    scaled_sequences = [scaled_data[i*sequences_shape[0]:(i+1)*sequences_shape[0]] for i in range(len(sequences))]\n",
    "    \n",
    "    return scaled_sequences\n",
    "\n",
    "# Apply the scaling function to your sequences\n",
    "scaled_sequences = scale_sequences(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the sequences to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_to_tensors(sequences, sequence_labels):\n",
    "    \"\"\"\n",
    "    Convert sequences and their labels into PyTorch tensors efficiently.\n",
    "\n",
    "    Parameters:\n",
    "    - sequences: List of np.arrays, the packet sequences.\n",
    "    - sequence_labels: List of int, the binary labels for each sequence.\n",
    "\n",
    "    Returns:\n",
    "    - sequence_tensors: PyTorch Tensor, the packet sequences as tensors.\n",
    "    - label_tensors: PyTorch Tensor, the labels for each sequence as tensors.\n",
    "    \"\"\"\n",
    "    # Stack the sequences into a single numpy array\n",
    "    sequences_array = np.stack(sequences)\n",
    "    \n",
    "    # Convert the stacked array to a PyTorch tensor\n",
    "    sequence_tensors = torch.tensor(sequences_array, dtype=torch.float32)\n",
    "    \n",
    "    # Convert labels to a tensor\n",
    "    label_tensors = torch.tensor(sequence_labels, dtype=torch.long)\n",
    "    \n",
    "    return sequence_tensors, label_tensors\n",
    "\n",
    "def binarize_labels(sequence_labels):\n",
    "    \"\"\"\n",
    "    Convert sequence labels from categorical ('Benign', 'Malware') to binary (0, 1).\n",
    "\n",
    "    Parameters:\n",
    "    - sequence_labels: List of str, the categorical labels for each sequence.\n",
    "\n",
    "    Returns:\n",
    "    - binary_labels: List of int, the binary labels for each sequence.\n",
    "    \"\"\"\n",
    "    binary_labels = [0 if label == 'Benign' else 1 for label in sequence_labels]\n",
    "    return binary_labels\n",
    "\n",
    "# Assuming `sequence_labels` contains your categorical labels\n",
    "binary_sequence_labels = binarize_labels(sequence_labels)\n",
    "\n",
    "# Now convert your sequences and the newly binarized labels into PyTorch tensors\n",
    "sequence_tensors, label_tensors = sequences_to_tensors(scaled_sequences, binary_sequence_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test datasets and instantiate DataLoader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        sequences: A tensor of shape [n_sequences, 16, 7] - packets per sequence and features per packet\n",
    "        labels: A tensor of labels for the sequences\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Optionally, add any sequence-specific preprocessing here\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sequence, label\n",
    "\n",
    "# 1. Splitting dataset into class-specific subsets\n",
    "sequences_0 = sequence_tensors[label_tensors == 0]\n",
    "labels_0 = label_tensors[label_tensors == 0]\n",
    "\n",
    "sequences_1 = sequence_tensors[label_tensors == 1]\n",
    "labels_1 = label_tensors[label_tensors == 1]\n",
    "\n",
    "# 2. Ensuring even class distribution for the validation set (similarly to the test set)\n",
    "min_count = min(len(sequences_0), len(sequences_1))\n",
    "# Using a portion of data for validation, let's say 20% of min_count\n",
    "val_count = int(0.2 * min_count)\n",
    "\n",
    "# 3. Randomly sampling for validation set\n",
    "perm_0_val = torch.randperm(len(sequences_0))[:val_count]\n",
    "perm_1_val = torch.randperm(len(sequences_1))[:val_count]\n",
    "\n",
    "# Creating balanced validation datasets\n",
    "sequences_val_balanced = torch.cat((sequences_0[perm_0_val], sequences_1[perm_1_val]), dim=0)\n",
    "labels_val_balanced = torch.cat((labels_0[perm_0_val], labels_1[perm_1_val]), dim=0)\n",
    "\n",
    "# 4. Updating train dataset after excluding validation samples\n",
    "sequences_0_train = torch.cat((sequences_0[val_count:], sequences_0[:val_count]), dim=0)[val_count:]\n",
    "labels_0_train = torch.cat((labels_0[val_count:], labels_0[:val_count]), dim=0)[val_count:]\n",
    "\n",
    "sequences_1_train = torch.cat((sequences_1[val_count:], sequences_1[:val_count]), dim=0)[val_count:]\n",
    "labels_1_train = torch.cat((labels_1[val_count:], labels_1[:val_count]), dim=0)[val_count:]\n",
    "\n",
    "# 5. Creating train, validation, and test datasets & loaders\n",
    "train_dataset = SequenceDataset(torch.cat((sequences_0_train, sequences_1_train), dim=0), torch.cat((labels_0_train, labels_1_train), dim=0))\n",
    "val_dataset = SequenceDataset(sequences_val_balanced, labels_val_balanced)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f'Shape of training data: {len(train_dataset)} {len(train_dataset[0])}')\n",
    "print(f'Shape of testing data: {len(val_dataset)} {len(val_dataset[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SequenceAutoencoder, self).__init__()\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=8, kernel_size=2, padding=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(2, stride=2),  # Intelligent pooling\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.encoder_lstm = nn.LSTM(input_size=8, hidden_size=8, batch_first=True)\n",
    "        \n",
    "        self.decoder_lstm = nn.LSTM(input_size=8, hidden_size=8, batch_first=True)\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=8, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.5)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder_conv(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)  # Adjusting for LSTM input\n",
    "        \n",
    "        # Assuming you're dealing with LSTM layers here...\n",
    "        x, (hn, cn) = self.encoder_lstm(x)\n",
    "        \n",
    "        x = self.decoder_conv(x.permute(0, 2, 1))  # Adjusting back for ConvTranspose1D\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = SequenceAutoencoder()\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # L2 regularization\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Learning rate decay\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=5, scheduler=None, patience=5):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0  # Initialize patience counter\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for sequences, _ in train_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, sequences)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for sequences, _ in val_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, sequences)\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Checkpoint and early stopping logic\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print('Model checkpoint saved')\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter\n",
    "        \n",
    "        if patience_counter > patience:\n",
    "            print(f'Stopping early at epoch {epoch+1}. No improvement in validation loss for {patience} epochs.')\n",
    "            break\n",
    "        \n",
    "        # Update learning rate\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(avg_val_loss)  # For ReduceLROnPlateau\n",
    "            else:\n",
    "                scheduler.step()  # For other types of schedulers\n",
    "    \n",
    "    print('Training complete')\n",
    "\n",
    "# Configuration and Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SequenceAutoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * 15\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=total_steps)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=5, scheduler=scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "evaluation = os.path.join(parent_dir, 'evaluation')\n",
    "sys.path.append(evaluation)\n",
    "\n",
    "from evaluate import AnomalyDetection # type: ignore\n",
    "\n",
    "evaluation = AnomalyDetection(model, train_loader, device)\n",
    "evaluation.run_SAE_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
